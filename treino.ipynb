{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游닌 Importa칞칚o das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游 Treinamento de Rede Neural com TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso necess치rio, converta o target para formato num칠rico (0/1) se ainda n칚o estiver\n",
    "# y_train = y_train.map({'no': 0, 'yes': 1})\n",
    "# y_test = y_test.map({'no': 0, 'yes': 1})\n",
    "def rede_neural(X_train, y_train, X_test, y_test):\n",
    "    # Defini칞칚o da arquitetura do modelo\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1], )),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Ajuste para problemas bin치rios\n",
    "    ])\n",
    "\n",
    "    # Compila칞칚o do modelo\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Treinamento do modelo\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=50, \n",
    "                        batch_size=32, \n",
    "                        validation_split=0.2)\n",
    "\n",
    "    # Avalia칞칚o do modelo no conjunto de teste\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Precis칚o da rede neural no conjunto de teste: {accuracy:.2f}\")\n",
    "\n",
    "    # (Opcional) Visualiza칞칚o dos resultados do treinamento\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['accuracy'], label='acur치cia treinamento')\n",
    "    plt.plot(history.history['val_accuracy'], label='acur치cia valida칞칚o')\n",
    "    plt.xlabel('칄poca')\n",
    "    plt.ylabel('Acur치cia')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游늷 Fun칞칚o de Ativa칞칚o Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游꿢 Fun칞칚o `predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"Realiza previs칚o usando uma rede neural de duas camadas\"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Adiciona uma coluna de bias (1s) na entrada\n",
    "    X = np.append(np.ones((m, 1)), X, axis=1)\n",
    "    \n",
    "    # Forward propagation - Camada oculta\n",
    "    z1 = np.dot(X, Theta1.T)\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # Adiciona coluna de bias na camada oculta\n",
    "    a1 = np.append(np.ones((m, 1)), a1, axis=1)\n",
    "\n",
    "    # Forward propagation - Camada de sa칤da\n",
    "    z2 = np.dot(a1, Theta2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return (a2 >= 0.5).astype(int)  # Ajustado para sa칤da bin치ria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游뱄 Treinamento de Rede Neural Manual com NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural2(X_train, y_train, X_test, y_test):\n",
    "    # Inicializa칞칚o aleat칩ria dos pesos da rede\n",
    "    print(X_train.shape[1])\n",
    "    input_layer_size = X_train.shape[1]  # N칰mero de features\n",
    "    hidden_layer_size = 25  # N칰mero arbitr치rio de neur칪nios na camada oculta\n",
    "    output_layer_size = 1  # Sa칤da bin치ria\n",
    "\n",
    "    # Pesos aleat칩rios entre -0.5 e 0.5\n",
    "    Theta1 = np.random.rand(hidden_layer_size, input_layer_size + 1) - 0.5\n",
    "    Theta2 = np.random.rand(output_layer_size, hidden_layer_size + 1) - 0.5\n",
    "\n",
    "    # Treinamento simples usando descida do gradiente\n",
    "    alpha = 0.01  # Taxa de aprendizado\n",
    "    epochs = 10000\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Forward propagation\n",
    "        m = X_train.shape[0]\n",
    "        X_bias = np.append(np.ones((m, 1)), X_train, axis=1)\n",
    "        z1 = np.dot(X_bias, Theta1.T)\n",
    "        a1 = sigmoid(z1)\n",
    "        a1_bias = np.append(np.ones((m, 1)), a1, axis=1)\n",
    "        z2 = np.dot(a1_bias, Theta2.T)\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        # C치lculo do erro\n",
    "        error = a2 - y_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Backpropagation - Ajuste dos pesos\n",
    "        dTheta2 = np.dot(error.T, a1_bias) / m\n",
    "        dTheta1 = np.dot(((error @ Theta2[:,1:]) * a1 * (1 - a1)).T, X_bias) / m\n",
    "\n",
    "        # Atualiza칞칚o dos pesos\n",
    "        Theta1 -= alpha * dTheta1\n",
    "        Theta2 -= alpha * dTheta2\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            y_train_reshaped = y_train.to_numpy().reshape(-1, 1)  # Converter para array 2D\n",
    "            loss = np.mean(-y_train_reshaped * np.log(a2) - (1 - y_train_reshaped) * np.log(1 - a2))\n",
    "            print(f\"칄poca {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "    # Avalia칞칚o do modelo\n",
    "    y_pred = predict(Theta1, Theta2, X_test)\n",
    "    accuracy = np.mean(y_pred.flatten() == y_test.values) * 100\n",
    "    print(f\"Acur치cia da rede neural: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
