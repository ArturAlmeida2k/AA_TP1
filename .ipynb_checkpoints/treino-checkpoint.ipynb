{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Importa√ß√£o das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Treinamento de Rede Neural com TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso necess√°rio, converta o target para formato num√©rico (0/1) se ainda n√£o estiver\n",
    "# y_train = y_train.map({'no': 0, 'yes': 1})\n",
    "# y_test = y_test.map({'no': 0, 'yes': 1})\n",
    "def rede_neural(X_train, y_train, X_test, y_test):\n",
    "    # Defini√ß√£o da arquitetura do modelo\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1], )),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Ajuste para problemas bin√°rios\n",
    "    ])\n",
    "\n",
    "    # Compila√ß√£o do modelo\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Treinamento do modelo\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=50, \n",
    "                        batch_size=32, \n",
    "                        validation_split=0.2)\n",
    "\n",
    "    # Avalia√ß√£o do modelo no conjunto de teste\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Precis√£o da rede neural no conjunto de teste: {accuracy:.2f}\")\n",
    "\n",
    "    # (Opcional) Visualiza√ß√£o dos resultados do treinamento\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['accuracy'], label='acur√°cia treinamento')\n",
    "    plt.plot(history.history['val_accuracy'], label='acur√°cia valida√ß√£o')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Acur√°cia')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Fun√ß√£o de Ativa√ß√£o Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Fun√ß√£o `predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"Realiza previs√£o usando uma rede neural de duas camadas\"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Adiciona uma coluna de bias (1s) na entrada\n",
    "    X = np.append(np.ones((m, 1)), X, axis=1)\n",
    "    \n",
    "    # Forward propagation - Camada oculta\n",
    "    z1 = np.dot(X, Theta1.T)\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # Adiciona coluna de bias na camada oculta\n",
    "    a1 = np.append(np.ones((m, 1)), a1, axis=1)\n",
    "\n",
    "    # Forward propagation - Camada de sa√≠da\n",
    "    z2 = np.dot(a1, Theta2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return (a2 >= 0.5).astype(int)  # Ajustado para sa√≠da bin√°ria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Treinamento de Rede Neural Manual com NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural2(X_train, y_train, X_test, y_test):\n",
    "    # Inicializa√ß√£o aleat√≥ria dos pesos da rede\n",
    "    print(X_train.shape[1])\n",
    "    input_layer_size = X_train.shape[1]  # N√∫mero de features\n",
    "    hidden_layer_size = 25  # N√∫mero arbitr√°rio de neur√¥nios na camada oculta\n",
    "    output_layer_size = 1  # Sa√≠da bin√°ria\n",
    "\n",
    "    # Pesos aleat√≥rios entre -0.5 e 0.5\n",
    "    Theta1 = np.random.rand(hidden_layer_size, input_layer_size + 1) - 0.5\n",
    "    Theta2 = np.random.rand(output_layer_size, hidden_layer_size + 1) - 0.5\n",
    "\n",
    "    # Treinamento simples usando descida do gradiente\n",
    "    alpha = 0.01  # Taxa de aprendizado\n",
    "    epochs = 10000 # N√∫mero de itera√ß√µes do treinamento\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Forward propagation\n",
    "        m = X_train.shape[0]\n",
    "\n",
    "        # Adiciona bias √† entrada\n",
    "        # Calcula a ativa√ß√£o da camada oculta\n",
    "        X_bias = np.append(np.ones((m, 1)), X_train, axis=1)\n",
    "        z1 = np.dot(X_bias, Theta1.T)\n",
    "        a1 = sigmoid(z1)\n",
    "\n",
    "        # Adiciona bias √† camada oculta\n",
    "        # Calcula a ativa√ß√£o da saida\n",
    "        a1_bias = np.append(np.ones((m, 1)), a1, axis=1)\n",
    "        z2 = np.dot(a1_bias, Theta2.T)\n",
    "        a2 = sigmoid(z2) # Sa√≠da final\n",
    "\n",
    "        # C√°lculo do erro\n",
    "        error = a2 - y_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "        # Backpropagation - Ajuste dos pesos\n",
    "        dTheta2 = np.dot(error.T, a1_bias) / m\n",
    "        dTheta1 = np.dot(((error @ Theta2[:,1:]) * a1 * (1 - a1)).T, X_bias) / m\n",
    "\n",
    "        # Atualiza√ß√£o dos pesos\n",
    "        Theta1 -= alpha * dTheta1\n",
    "        Theta2 -= alpha * dTheta2\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            y_train_reshaped = y_train.to_numpy().reshape(-1, 1)  # Converter para array 2D\n",
    "            loss = np.mean(-y_train_reshaped * np.log(a2) - (1 - y_train_reshaped) * np.log(1 - a2))\n",
    "            print(f\"√âpoca {i}: Loss = {loss:.4f}\") # Imprime o erro a cada 100 √©pocas\n",
    "\n",
    "    # Avalia√ß√£o do modelo\n",
    "    y_pred = predict(Theta1, Theta2, X_test)\n",
    "\n",
    "    # Calcula a acur√°cia\n",
    "    accuracy = np.mean(y_pred.flatten() == y_test.values) * 100\n",
    "    print(f\"Acur√°cia da rede neural: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
